//! Pass 1-n
#version 450

#extension GL_GOOGLE_include_directive : require

layout(set = 0, binding = 0) readonly buffer B_ContreeBuildState {
    uint prev_dim;
    uint curr_dim;
    uint level;
}
contree_build_state;

layout(set = 0, binding = 1) readonly buffer B_NodeOffsetForLevels { uint data[]; }
node_offset_for_levels;

#include "../../include/contree_node.glsl"
layout(set = 0, binding = 2) buffer B_SparseNodes { ContreeNode data[]; }
sparse_nodes;

layout(set = 0, binding = 3) writeonly buffer B_DenseNodes { ContreeNode data[]; }
dense_nodes;

layout(set = 0, binding = 4) buffer B_CounterForLevels { uint data[]; }
counter_for_levels;

layout(set = 0, binding = 5) buffer B_ContreeBuildResult {
    uint node_len;
    uint leaf_len;
}
contree_build_result;

layout(local_size_x = 4, local_size_y = 4, local_size_z = 4) in;

#include "../../include/core/bits.glsl"

void main() {
    ivec3 uvi = ivec3(gl_GlobalInvocationID);
    if (any(greaterThanEqual(uvi, ivec3(contree_build_state.curr_dim)))) {
        return;
    }

    ContreeNode temp[64];
    uint64_t child_mask = 0ul;

    for (uint yi = 0; yi < 4; yi++) {
        for (uint zi = 0; zi < 4; zi++) {
            for (uint xi = 0; xi < 4; xi++) {
                uint i = xi + zi * 4 + yi * 16;

                uvec3 vpos = uvec3(uvi) * 4 + uvec3(xi, yi, zi);

                uint prev_sparse_node_read_offset =
                    node_offset_for_levels.data[contree_build_state.level + 1];
                uint prev_idx =
                    uint(vpos.x) + uint(vpos.z) * contree_build_state.prev_dim +
                    uint(vpos.y) * contree_build_state.prev_dim * contree_build_state.prev_dim;
                prev_idx += prev_sparse_node_read_offset;
                ContreeNode cn = sparse_nodes.data[prev_idx];
                temp[i]        = cn;

                // the node is valid if non-leaf, or child_mask != 0
                bool exists = (cn.child_mask != 0ul) || ((cn.packed_0 & 1u) != 0u);
                child_mask |= exists ? (uint64_t(1) << i) : 0ul;
            }
        }
    }

    uint cnt = bit_count_u64(child_mask);

    uint sparse_node_write_offset = node_offset_for_levels.data[contree_build_state.level];
    uint brick_idx                = uint(uvi.x) + uint(uvi.z) * contree_build_state.curr_dim +
                     uint(uvi.y) * contree_build_state.curr_dim * contree_build_state.curr_dim;
    brick_idx += sparse_node_write_offset;

    // early out: if no voxels are presented in this brick
    if (cnt == 0) {
        sparse_nodes.data[brick_idx].packed_0   = 0;
        sparse_nodes.data[brick_idx].child_mask = 0ul;
        return;
    }

    // summarize it to the overall valid node count buffer, this is useful for guiding the concat
    // pass
    atomicAdd(contree_build_result.node_len, cnt);

    // allocate a region for the leaf data to be written
    uint base   = atomicAdd(counter_for_levels.data[contree_build_state.level + 1], cnt);
    uint offset = node_offset_for_levels.data[contree_build_state.level + 1];
    uint w      = 0;
    for (uint i = 0; i < 64; i++) {
        if ((child_mask & (uint64_t(1) << i)) != 0) {
            dense_nodes.data[offset + base + w] = temp[i];
            w++;
        }
    }

    // because we are using 31 bits for the leaf ptr, there's a limit of 1290^3 of total voxels pre
    // chunk
    // 1290^3 = 2146689000, 2^31 = 2147483648, so we are safe
    // so the maximum available voxel dim limit is 1024^3, to match the 4^n dimension need of
    // contree
    sparse_nodes.data[brick_idx].packed_0   = (0u) | (base << 1);
    sparse_nodes.data[brick_idx].child_mask = child_mask;
}
