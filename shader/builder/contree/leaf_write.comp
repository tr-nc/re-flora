#version 450

#extension GL_GOOGLE_include_directive : require

layout(local_size_x = 4, local_size_y = 4, local_size_z = 4) in;

layout(set = 0, binding = 0) readonly buffer B_ContreeBuildState {
    uint prev_dim;
    uint curr_dim;
    uint level;
}
contree_build_state;

layout(set = 0, binding = 1, r32ui) readonly uniform uimage3D frag_img;

layout(set = 0, binding = 2) readonly buffer B_NodeOffsetForLevels { uint data[]; }
node_offset_for_levels;

#include "../../include/contree_node.glsl"
layout(set = 0, binding = 3) writeonly buffer B_SparseNodes { ContreeNode data[]; }
sparse_nodes;

layout(set = 0, binding = 4) writeonly buffer B_LeafData { uint data[]; }
leaf_data;

layout(set = 0, binding = 5) writeonly buffer B_ContreeBuildResult {
    uint node_len;
    uint leaf_len;
}
contree_build_result;

#include "../../include/core/bits.glsl"

// Shared memory for workgroup-local operations. Size is 64 (4*4*4).
// s_voxel_counts will be used for the parallel prefix sum.
shared uint s_voxel_counts[64];
// s_workgroup_total_voxels_in_group stores the sum of voxels for this entire workgroup.
shared uint s_workgroup_total_voxels_in_group;
// s_base_offset_for_workgroup_leaf_data stores the starting offset in the global leaf_data buffer
// for this workgroup.
shared uint s_base_offset_for_workgroup_leaf_data;

void main() {
    ivec3 uvi = ivec3(gl_GlobalInvocationID);
    if (any(greaterThanEqual(uvi, ivec3(contree_build_state.curr_dim)))) {
        return;
    }

    uint local_id = gl_LocalInvocationIndex;

    uint temp[64];
    uint64_t child_mask = 0;
    for (uint yi = 0; yi < 4; yi++) {
        for (uint zi = 0; zi < 4; zi++) {
            for (uint xi = 0; xi < 4; xi++) {
                uint i = xi + zi * 4 + yi * 16;

                ivec3 vpos = uvi * 4 + ivec3(xi, yi, zi);
                uint v     = imageLoad(frag_img, vpos).x;
                temp[i]    = v;

                // the node is valid if voxel type is non zero
                bool exists = (v != 0);
                child_mask |= exists ? (uint64_t(1) << i) : 0ul;
            }
        }
    }

    uint cnt = bit_count_u64(child_mask);

    // MARK:

    // Store this invocation's count of valid voxels into shared memory.
    s_voxel_counts[local_id] = cnt;
    // Synchronize all invocations in the workgroup to ensure all counts are written to shared
    // memory before proceeding to the scan.
    barrier();

    // --- Begin Parallel Exclusive Scan (Blelloch scan algorithm) ---
    // This scan operates on s_voxel_counts. After the scan, s_voxel_counts[local_id]
    // will hold the sum of all 'cnt' values from invocations with local indices less than local_id.
    // The total sum for the workgroup will be available in s_workgroup_total_voxels_in_group.

    // 1. Up-sweep phase (Reduction phase):
    // Iteratively sums values. After this loop, s_voxel_counts[local_id] holds the inclusive sum
    // of 'cnt' from local_id 0 up to the current local_id.
    // The last element (s_voxel_counts[63]) will hold the total sum for the workgroup.
    for (uint d = 0; d < 6; ++d) { // log2(64) = 6 iterations for workgroup size 64
        uint offset = 1 << d;
        if (local_id >= offset) {
            s_voxel_counts[local_id] += s_voxel_counts[local_id - offset];
        }
        // Synchronize after each step of the up-sweep to ensure correct dependencies.
        barrier();
    }

    // The last invocation (local_id == 63) now has the total sum of voxels for this workgroup in
    // its s_voxel_counts[63]. Store this total sum into s_workgroup_total_voxels_in_group. For an
    // exclusive scan, the prefix sum for the last element is effectively 0 relative to its own
    // value's contribution to subsequent global offsets, so set s_voxel_counts[63] to 0.
    if (local_id == (64 - 1)) {
        s_workgroup_total_voxels_in_group = s_voxel_counts[local_id];
        s_voxel_counts[local_id]          = 0;
    }
    // Synchronize to make s_workgroup_total_voxels_in_group visible to all and ensure
    // s_voxel_counts[63] is updated.
    barrier();

    // 2. Down-sweep phase:
    // Converts the inclusive sums from the up-sweep phase into exclusive prefix sums.
    for (int d = 5; d >= 0; --d) { // Iterate downwards from log2(64)-1
        uint offset = 1 << d;
        if (local_id >= offset) {
            uint temp                         = s_voxel_counts[local_id - offset];
            s_voxel_counts[local_id - offset] = s_voxel_counts[local_id];
            s_voxel_counts[local_id] += temp;
        }
        // Synchronize after each step of the down-sweep.
        barrier();
    }
    // --- End Parallel Exclusive Scan ---

    // s_voxel_counts[local_id] now holds the exclusive prefix sum for this invocation.
    // This is the offset of this invocation's leaf data *within the workgroup's* aggregated chunk.
    uint local_exclusive_prefix_sum = s_voxel_counts[local_id];

    // One invocation from the workgroup (e.g., the last one) performs the global atomic add.
    // It adds the total count of voxels for the entire workgroup
    // (s_workgroup_total_voxels_in_group) to the global leaf_len counter. The returned value is the
    // base offset in the global leaf_data buffer for this workgroup's combined data.
    if (local_id == (64 - 1)) {
        s_base_offset_for_workgroup_leaf_data =
            atomicAdd(contree_build_result.leaf_len, s_workgroup_total_voxels_in_group);
    }
    // Synchronize to make s_base_offset_for_workgroup_leaf_data (written by one invocation)
    // visible to all other invocations in the workgroup.
    barrier();

    // Calculate the final base address in leaf_data for this specific invocation.
    // It's the sum of the workgroup's base offset and this invocation's local offset within the
    // workgroup.
    uint base = s_base_offset_for_workgroup_leaf_data + local_exclusive_prefix_sum;

    // MARK:

    uint sparse_node_write_offset = node_offset_for_levels.data[contree_build_state.level];
    uint brick_idx                = uint(uvi.x) + uint(uvi.z) * contree_build_state.curr_dim +
                     uint(uvi.y) * contree_build_state.curr_dim * contree_build_state.curr_dim;
    brick_idx += sparse_node_write_offset;

    // early out: if no voxels are presented in this brick
    if (cnt == 0) {
        sparse_nodes.data[brick_idx].packed_0   = 0;
        sparse_nodes.data[brick_idx].child_mask = 0ul;
        return;
    }

    // If there are valid voxels, write their data to the leaf_data buffer.
    // 'w' tracks the number of voxels written by this invocation.
    uint w = 0;
    for (uint i = 0; i < 64; i++) {
        // Check if the i-th voxel in the 4x4x4 brick is valid.
        if ((child_mask & (uint64_t(1) << i)) != 0) {
            leaf_data.data[base + w] = temp[i];
            w++;
        }
    }

    // because we are using 31 bits for the leaf ptr, there's a limit of 1290^3 of total voxels pre
    // chunk
    // 1290^3 = 2146689000, 2^31 = 2147483648, so we are safe
    // so the maximum available voxel dim limit is 1024^3, to match the 4^n dimension need of
    // contree
    sparse_nodes.data[brick_idx].packed_0   = 1 | (base << 1);
    sparse_nodes.data[brick_idx].child_mask = child_mask;
}
